{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions Using the Naive Bayes Algorithm\n",
    "*Curtis Miller*\n",
    "\n",
    "The **naive Bayes** algorithm is based on Bayes' theorem. Let's suppose that our data uses features $X_1, X_2, ..., X_K$ and $Y$ is the target variable. One form to describe the naive Bayes classifier is:\n",
    "\n",
    "$$P(Y = y | X_1 = x_1, ..., X_K = x_K) = \\frac{P(X_1 = x_1, ..., X_K = x_K | Y = y) P(Y = y)}{P(X_1 = x_1, ..., X_K = x_K)}$$\n",
    "\n",
    "The \"naive\" part of the naive Bayes classifier is to make the (unrealistic) assumption that all the features are **independent** random variables; that is, information about one feature provides essentially no information about the other, and $P(X_i = x_i | X_j = x_j) = P(X_i = x_i)$ when $i \\neq j$ (but we *don't* assume the features are independent of the target variable). Under this assumption, $P(X_i = x_i, X_j = x_j) = P(X_i = x_i)P(X_j = x_j)$ and we can rewrite Bayes theorem as\n",
    "\n",
    "$$P(Y = y | X_1 = x_1, ..., X_K = x_K) = \\frac{P(Y = y) \\prod_{k = 1}^{K} P(X_k = x_k | Y = y)}{\\prod_{k = 1}{K} P(X_k = x_k)} \\propto P(Y = y) \\prod_{k = 1}^{K} P(X_k = x_k | Y = y)$$\n",
    "\n",
    "The quantities in the above expression (behind the $\\propto$ symbol) are easily estimated from a dataset; this is what is done when training the algorithm.\n",
    "\n",
    "When predicting, we observe features $x_1, ..., x_K$ for a data point. The predicted label $y$ is the label that maximizes $P(Y = y) \\prod_{k = 1}^{K} P(X_k = x_k | Y = y)$; that is, if $\\hat{y}$ represents our prediction, then\n",
    "\n",
    "$$\\hat{y} = \\arg \\max_y P(Y = y) \\prod_{k = 1}^{K} P(X_k = x_k | Y = y)$$\n",
    "\n",
    "Notice that what I have written works only when $X_k$ are all discrete; this algorithm will work for categorical features (like passenger class or sex) or variables that take countable values (like number of children aboard), but it won't work for features we may consider continuous (like fare paid) since in that case $P(X_k = x_k | Y = y)$ always. We fix this problem by replacing, for continuous features, $P(X_k = x_k | Y = y)$ with $f_k(x_k | y)$, where $f( \\cdot )$ is a probability density function. A common choice is the Gaussian density:\n",
    "\n",
    "$$f_k(x_k | y) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2_{k,y}}} \\exp\\left(-\\frac{(x_k - \\mu_{k,y})^2}{2 \\sigma_{k,y}^2}\\right)$$\n",
    "\n",
    "(Note that $\\exp(x) = e^x$.) The parameters $\\mu_{k,y}$ and $\\sigma_{k,y}$ are estimated from the data.\n",
    "\n",
    "If $U_1, ..., U_I$ are discrete variables and $V_1, ..., V_J$ continuous, we can rewrite the naive Bayes classifier like so:\n",
    "\n",
    "$$\\hat{y} = \\arg \\max_y P(Y = y) \\prod_{i = 1}^{I} P(U_i = u_i | Y = y) \\prod_{j = 1}^{J} f_j(v_j | y)$$\n",
    "\n",
    "Hyperparameters for the naive Bayes algorithm are the prior distributions for all features and probabilities mentioned here, before observing data. In particular we may want to choose a prior probability for $P(Y = y)$.\n",
    "\n",
    "In this notebook I will be implementing Bernoulli naive Bayes, where we don't allow for any continuous random variables; every variable takes one of two values. This means we will need to implement binning for continuous variables and break up count variables into each observed count. This requires preprocessing the data. The choice of bins also acts like a hyperparameter.\n",
    "\n",
    "Bernoulli naive Bayes is implemented in **scikit-learn** through the `BernoulliNB` object.\n",
    "\n",
    "## Linear Separability\n",
    "\n",
    "KNN, decision trees, and decision forests don't assume much about the underlying data, but from this point on the classifiers I consider (including the naive Bayes algorithm) assume that the data is **linearly separable**. That is, data with different labels can be separated by a straight line in 2D space, or a hyperplane in N-dimensional space (a hyperplane is a general notion of a line). Data is usually considered to be lying in a space that has more than even three dimensions If this assumption is violated, the algorithm may struggle, if not fail outright, to correctly learn and classify the data.\n",
    "\n",
    "In general it's difficult to check whether data is linearly separable. Visualization may be useful for determining this.\n",
    "\n",
    "## Preprocessing the Data\n",
    "\n",
    "We will continue our work with the *Titanic* dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"titanic.csv\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we would model `Pclass`, `Sex`, `Siblings/Spouses Aboard`, and `Parents/Children Aboard` as discrete variable, while `Age` and `Fare` should be considered continuous. We will need to bin `Age` and `Fare` in order to be able to use `BernoulliNB`. Our work with decision trees may suggest what bins to use (we may also want to use cross-validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(titanic.Age, bins=[-1, 2, titanic.Age.max() + 1]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.cut(titanic.Fare, bins=[0, 23.35, titanic.Fare.max() + 1]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = titanic.assign(Age_cat=(titanic.Age <= 2), Fare_cat=(titanic.Fare <= 23.35))\n",
    "titanic.replace({'Sex': {'male': 0, 'female': 1}}, inplace=True)\n",
    "titanic.drop(['Age', 'Fare', 'Name'], axis=1, inplace=True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train, titanic_test = train_test_split(titanic)\n",
    "titanic_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Naive Bayes Algorithm\n",
    "\n",
    "Let's now fit a Bernoulli naive Bayes algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb = BernoulliNB(alpha=0,    # Additive smoothing parameter; setting to 0 for no smoothing\n",
    "                  fit_prior=False,     # Don't learn a prior distribution for the label\n",
    "                  class_prior=None)    # Don't have prior distributions for features\n",
    "bnb = bnb.fit(titanic_train.drop(\"Survived\", axis=1), titanic_train.Survived)\n",
    "print(classification_report(titanic_train.Survived, bnb.predict(titanic_train.drop(\"Survived\", axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the algorithm's predictive accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_test_predict = bnb.predict(titanic_test.drop(\"Survived\", axis=1))\n",
    "print(classification_report(titanic_test.Survived, survived_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-sample and out-of-sample performance for this dataset are similar. That demonstrates one feature of linear models that data scientists like: linear models tend to generalize well.\n",
    "\n",
    "That said, let's see if we can get better performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
